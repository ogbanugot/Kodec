{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25620,
     "status": "ok",
     "timestamp": 1595822167230,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "cSLzI4c3u1YO",
    "outputId": "01e47d02-0680-4a3d-9e79-8337e68f88b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import csv\n",
    "from drive.minisom.minisom import MiniSom\n",
    "from sklearn.metrics import accuracy_score, normalized_mutual_info_score\n",
    "import drive.DEC.metrics as met\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2543,
     "status": "ok",
     "timestamp": 1595822176006,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "3mZM9kFBu36G",
    "outputId": "0a3048c8-c0a7-4411-af54-3bfa4df802b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y), (val_x, val_y) = mnist.load_data()\n",
    " \n",
    "train_x = train_x.astype('float32') / 255.\n",
    "val_x = val_x.astype('float32') / 255.\n",
    "train_x = train_x.reshape((len(train_x), np.prod(train_x.shape[1:])))\n",
    "val_x = val_x.reshape((len(val_x), np.prod(val_x.shape[1:])))\n",
    "print(train_x.shape)\n",
    "print(val_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1297,
     "status": "ok",
     "timestamp": 1595822177339,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "fiYOl2URlAnC"
   },
   "outputs": [],
   "source": [
    "train_x = np.concatenate((train_x, val_x),0)\n",
    "train_y = np.concatenate((train_y, val_y),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1398,
     "status": "ok",
     "timestamp": 1595588863132,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "lwF50GIRvDyx",
    "outputId": "e3a6e085-751d-43a8-ecb8-0975cc2578d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1595822178889,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "pYk6C2rrJcIF"
   },
   "outputs": [],
   "source": [
    "class SelfOrganizingMaps():   \n",
    "    def __init__(self,x,y,input_len,sigma,lr,num_iteration,random_seed):\n",
    "        self.model = MiniSom(x = x, y = y, input_len = input_len, sigma = sigma, learning_rate = lr, random_seed=random_seed)\n",
    "        self.num_iteration = num_iteration\n",
    "        \n",
    "    def train(self, data):    \n",
    "        self.model.train_random(data = data, num_iteration = self.num_iteration)\n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, valx, trainx, trainy, classes):\n",
    "        mappings = self.model.labels_map(trainx, trainy)\n",
    "        dataXpred  = []\n",
    "        for i in range(len(valx)):\n",
    "            wt = self.model.winner(valx[i])\n",
    "            cluster = mappings[wt]            \n",
    "            clssCount = []                      \n",
    "            for cls in classes: \n",
    "              clssCount.append(cluster.count(cls))  \n",
    "            Predictlabel = np.argmax(clssCount)\n",
    "            dataXpred.append(Predictlabel)                    \n",
    "        return dataXpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 107893,
     "status": "ok",
     "timestamp": 1595468264424,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "dFrQb90EmPeT",
    "outputId": "24bc39c2-c4a3-438f-a958-e57998d11e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time:  107.10339426994324\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(val_y)\n",
    "t0 = time()\n",
    "# Training the PCA + SOM\n",
    "som = SelfOrganizingMaps(x = 30, y = 30, input_len = 784, sigma = 1.0, \n",
    "                         lr = 0.5, num_iteration= 5000, random_seed=56)\n",
    "som.model.random_weights_init(train_x)\n",
    "som.train(train_x)\n",
    "print('training time: ', time() - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ja2nxSVVspr"
   },
   "outputs": [],
   "source": [
    "wt = som.model.winner(val_x)\n",
    "print(wt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 243832,
     "status": "ok",
     "timestamp": 1594874231456,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "9mkEfU1NN__X",
    "outputId": "97845d1b-d584-4570-b53a-db8e3f5ee05c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:  242.69993829727173\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "pred = som.predict(val_x, train_x, train_y, classes)\n",
    "print('predict time: ', time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1594874289232,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "YZ8Tft0akzR5",
    "outputId": "41304e41-8096-4a3c-9597-221d4ac310c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 0.7012943835484433\n",
      "ACC: 0.8254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"NMI:\", normalized_mutual_info_score(val_y, pred))\n",
    "pred = np.array(pred)\n",
    "print(\"ACC:\", met.acc(val_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1124,
     "status": "ok",
     "timestamp": 1595822182306,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "c9f_2Or3u1Yo"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(dims[0],dims[1]),        \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(dims[1],dims[2]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(dims[2],dims[3]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(dims[3],dims[4]),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    " \n",
    "    def forward(self,x):\n",
    "        x=self.model(x)    \n",
    "        return x\n",
    " \n",
    " \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(dims[4],dims[3]),        \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(dims[3],dims[2]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(dims[2],dims[1]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(dims[1],dims[0]),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.model(x)    \n",
    "        return x\n",
    " \n",
    "class Autoencoder(nn.Module):   \n",
    "    def __init__(self, dims, init):        \n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.encoder= Encoder(dims)\n",
    "        self.decoder= Decoder(dims)\n",
    "        if init is not None:\n",
    "          self.initialize_weights(init)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def initialize_weights(self, init):\n",
    "        if init == \"glorot_uniform\":\n",
    "          range = [0,2,4,6]\n",
    "          for i in range:\n",
    "            nn.init.xavier_normal_(self.encoder.model[i].weight.data)\n",
    "            nn.init.xavier_normal_(self.decoder.model[i].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9FA_ANPRu1Y-"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "epochs = 500\n",
    "batch_size = 2048\n",
    "if torch.cuda.is_available()==True:\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device =\"cpu\"\n",
    "\n",
    "dims=[train_x.shape[-1], 500, 500, 2000, 10]\n",
    "AE = Autoencoder(dims, init=None)\n",
    "AE.to(device)\n",
    "criterion=nn.MSELoss(reduction='mean')\n",
    "optimizer=optim.Adam(AE.parameters())\n",
    "train = torch.Tensor(train_x)\n",
    "trainloader=DataLoader(train_x, batch_size=batch_size)     \n",
    "\n",
    "'''kwargs = {'num_workers': 1, 'pin_memory': True} if device==\"cuda:0\" else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)'''\n",
    "\n",
    "t0 = time()\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for i, (data)  in enumerate(trainloader):\n",
    "        data = data.to(device)\n",
    "        #Forward Pass\n",
    "        output= AE(data)\n",
    "        loss=criterion(output, data)\n",
    "        #Backward Pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(\"======> epoch: {}/{}, Loss:{}\".format(epoch,epochs,train_loss))\n",
    "print('Pretraining time: ', time() - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "igxe__QylP3d"
   },
   "outputs": [],
   "source": [
    "def eval_predict(model,loader):\n",
    "  model.eval()\n",
    "  all_predict = torch.tensor([], device=device)\n",
    "  with torch.no_grad():\n",
    "      for i, (data) in enumerate(loader):\n",
    "          data = data.to(device)\n",
    "          predict = model(data)\n",
    "          all_predict = torch.cat((all_predict, predict), 0)\n",
    "  return all_predict \n",
    "  \n",
    "trainX = eval_predict(AE.encoder, trainloader).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57305,
     "status": "ok",
     "timestamp": 1595213797345,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "JYWNbM2AGI29",
    "outputId": "825c0746-6b48-4ff5-d884-81cdbc78fa4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time:  55.02388286590576\n"
     ]
    }
   ],
   "source": [
    "# Training the SOM without PCA initialization\n",
    "t0 = time()\n",
    "som = SelfOrganizingMaps(x = 30, y = 30, input_len = 10, sigma = 1.0, \n",
    "                         lr = 0.5, num_iteration= 5000, random_seed=56)\n",
    "som.model.random_weights_init(trainX)\n",
    "som.train(trainX)\n",
    "print('training time: ', time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 223766,
     "status": "ok",
     "timestamp": 1595214027000,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "jyJK0dIsGI3h",
    "outputId": "cc08f598-a1e9-4caa-d179-c5f83e1c778b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:  221.85195016860962\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(train_y)\n",
    "t0 = time()\n",
    "pred = som.predict(trainX, trainX, train_y, classes)\n",
    "print('predict time: ', time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "ok",
     "timestamp": 1595214096498,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "xPW73IIMGI3_",
    "outputId": "e9b24221-58d0-4f36-de2a-41bf7d21a1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 0.8000848249845998\n",
      "ACC: 0.9038833333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"NMI:\", normalized_mutual_info_score(train_y, pred))\n",
    "pred = np.array(pred)\n",
    "print(\"ACC:\", met.acc(train_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1132,
     "status": "ok",
     "timestamp": 1595822189506,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "_pllK38Tu1ZP"
   },
   "outputs": [],
   "source": [
    "class ClusteringLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    " \n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters, n_features, weights=None, alpha=1.0):        \n",
    "        super(ClusteringLayer, self).__init__()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        if weights is not None:\n",
    "            self.weights = nn.Parameter(weights)\n",
    "        else:\n",
    "            weights = torch.Tensor(self.n_clusters, self.n_features)\n",
    "            self.weights = nn.Parameter(weights)     \n",
    "            \n",
    "        nn.init.xavier_normal_(self.weights)\n",
    "        \n",
    "    def set_weights(self, weights):\n",
    "        self.weights = nn.Parameter(weights)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        input = inputs.clone().unsqueeze_(-1)\n",
    "        input = input.reshape(input.size()[0],1,self.n_features)\n",
    "        square = torch.square(input - self.weights)\n",
    "        q = 1.0 / (1.0 + (square.sum(dim=2) / self.alpha))\n",
    "        q = torch.pow(q, (self.alpha + 1.0) / 2.0)        \n",
    "        q = torch.transpose(torch.transpose(q, 0, 1) / q.sum(dim=1), 0, 1)\n",
    "        return q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1122,
     "status": "ok",
     "timestamp": 1595822192496,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "CJgwnMJ6u1Zj"
   },
   "outputs": [],
   "source": [
    "class DEC(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 n_clusters=900,\n",
    "                 alpha=1.0,\n",
    "                 init='glorot_uniform'):\n",
    " \n",
    "        super(DEC, self).__init__()        \n",
    "        self.dims = dims\n",
    "        self.n_features = dims[4]\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        encoder = Encoder(self.dims)\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, self.n_features)\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module(\"encoder\", encoder)\n",
    "        self.model.add_module(\"clustering_layer\", clustering_layer)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1760,
     "status": "ok",
     "timestamp": 1595637629746,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "87wOoG0THIeH"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    " \n",
    "class SimpleCustomBatch:\n",
    "    def __init__(self, data):\n",
    "        transposed_data = list(zip(*data))\n",
    "        self.inp = torch.stack(transposed_data[0], 0)\n",
    "        self.tgt = torch.stack(transposed_data[1], 0)\n",
    " \n",
    "    # custom memory pinning method on custom type\n",
    "    def pin_memory(self):\n",
    "        self.inp = self.inp.pin_memory()\n",
    "        self.tgt = self.tgt.pin_memory()\n",
    "        return self\n",
    " \n",
    "def collate_wrapper(batch):\n",
    "    return SimpleCustomBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2494,
     "status": "ok",
     "timestamp": 1595823652380,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "EjSmaq2Ou1Z1"
   },
   "outputs": [],
   "source": [
    "class DEC_Module():\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 n_clusters,\n",
    "                 alpha=1.0,\n",
    "                 init='glorot_uniform'):\n",
    "        \n",
    "      if torch.cuda.is_available()==True:\n",
    "          self.device=\"cuda\"\n",
    "      else:\n",
    "          self.device =\"cpu\"\n",
    "      self.AE = Autoencoder(dims, init=init)\n",
    "      self.dec = DEC(dims, n_clusters, init)\n",
    "    \n",
    "    def pretrain(self, x, y=None, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp'):\n",
    "      print('...Pretraining...') \n",
    "      device = self.device\n",
    "      self.AE.to(device)\n",
    "      criterion=nn.MSELoss()\n",
    "      optimizer=optim.Adam(self.AE.parameters())\n",
    "      trainloader=DataLoader(x, batch_size=batch_size)     \n",
    "      \n",
    "      t0 = time()\n",
    "      for epoch in range(epochs):\n",
    "          train_loss = 0\n",
    "          for i, (data)  in enumerate(trainloader):\n",
    "              data = data.to(device)\n",
    "              #Forward Pass\n",
    "              output=self.AE(data)\n",
    "              loss=criterion(output, data)\n",
    "              #Backward Pass\n",
    "              optimizer.zero_grad()\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "              \n",
    "          print(\"======> epoch: {}/{}, Loss:{}\".format(epoch,epochs,loss.item()))\n",
    "          if y is not None:\n",
    "              self.print_on_epoch_end(y, epoch,epochs, trainloader)\n",
    "      print('Pretraining time: ', time() - t0)\n",
    "      self.set_encoder_weights(self.AE.encoder.state_dict())\n",
    "      torch.save(self.AE.encoder.state_dict(), save_dir + '/encoder_weights.h5')\n",
    "      print('Pretrained weights are saved to %s/encoder_weights.h5' % save_dir)\n",
    "      self.pretrained = True\n",
    " \n",
    "    def print_on_epoch_end(self,y, epoch, epochs, loader):            \n",
    "      if epoch % int(epochs/10) != 0:\n",
    "          return\n",
    "      features = self.eval_predict(self.AE.encoder, loader).cpu().numpy()\n",
    " \n",
    "      '''classes = np.unique(y)                    \n",
    "      som = SelfOrganizingMaps(x = 30, y = 30, input_len = len(classes), sigma = 1.0, \n",
    "            lr = 0.5, num_iteration= 1,random_seed=56)\n",
    "      som.model.random_weights_init(features)\n",
    "      som.train(features)\n",
    "      y_pred = som.predict(features, features, y, classes)'''\n",
    "      km = KMeans(n_clusters=len(np.unique(y)), n_init=20, n_jobs=4)\n",
    "      y_pred = km.fit_predict(features)\n",
    "      y_pred = np.array(y_pred)\n",
    "      print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "            % (met.acc(y, y_pred), met.nmi(y, y_pred)))       \n",
    " \n",
    "    def extract_features(self, x):\n",
    "      loader=DataLoader(x, batch_size=self.batch_size)                \n",
    "      features = self.eval_predict(self.dec.AE.encoder, loader)\n",
    "      features = features.cpu().numpy()\n",
    "      return features\n",
    " \n",
    "    def predict(self, loader): #Output from clustering layer \n",
    "      q = self.eval_predict(self.dec.model, loader)\n",
    "      q = q.cpu().numpy()\n",
    "      return q.argmax(1)\n",
    " \n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "      weight = torch.square(q) / q.sum(0)\n",
    "      return torch.transpose(torch.transpose(weight,0,1)/weight.sum(1),0,1)\n",
    " \n",
    "    def eval_predict(self, model,loader):\n",
    "      device = self.device\n",
    "      model.eval()\n",
    "      all_predict = torch.tensor([], device=device)\n",
    "      with torch.no_grad():\n",
    "          for i, data in enumerate(loader):\n",
    "              data = data.to(device)\n",
    "              predict = model(data)\n",
    "              all_predict = torch.cat((all_predict, predict), 0)\n",
    "      return all_predict \n",
    "\n",
    "    def set_encoder_weights(self, state_dict):\n",
    "      self.dec.model.encoder.load_state_dict(state_dict)\n",
    "    \n",
    "    def set_clustering_weights(self, weights):\n",
    "      weights = torch.Tensor(weights)\n",
    "      self.dec.model.clustering_layer.set_weights(weights.to(self.device))\n",
    "    \n",
    "    def train(self, x, y=None, maxiter=5e4, batch_size=256, tol=1e-3,\n",
    "            update_interval=140, save_dir='./results/temp'):\n",
    "      print('Update interval', update_interval)\n",
    "      save_interval = x.shape[0] / batch_size * 5\n",
    "      print('Save interval', save_interval)\n",
    "      x = torch.Tensor(x)\n",
    "      # Step 1: initialize cluster centers using SOM\n",
    "      t1 = time()\n",
    "      print('Initializing cluster centers with SOM.')\n",
    "      self.dec.model.to(self.device)\n",
    "      trainloader=DataLoader(x, batch_size=batch_size)\n",
    "      #train_x = self.eval_predict(self.AE.encoder, trainloader).cpu().numpy()\n",
    "      #self.set_encoder_weights(self.AE.encoder.state_dict())\n",
    "      train_x = self.eval_predict(self.dec.model.encoder, trainloader).cpu().numpy()\n",
    "      classes = np.unique(y)                    \n",
    "      '''som = SelfOrganizingMaps(x = 30, y = 30, input_len = 10, sigma = 1.0, \n",
    "            lr = 0.5, num_iteration= 5000,random_seed=56)\n",
    "      som.model.random_weights_init(train_x)\n",
    "      som.train(train_x)\n",
    "      som_weights = som.model.get_weights()\n",
    "      som_weights = som_weights.reshape(900,10)\n",
    "      self.set_clustering_weights(som_weights)'''\n",
    "\n",
    "      kmeans = KMeans(n_clusters=len(classes), n_init=20)\n",
    "      y_pred = kmeans.fit_predict(train_x)\n",
    "      clster = kmeans.cluster_centers_\n",
    "      self.set_clustering_weights(clster)\n",
    "      \n",
    "      #initial soft probability predictions\n",
    "      q = self.eval_predict(self.dec.model, trainloader)\n",
    "      #initial target distribution p\n",
    "      p = self.target_distribution(q)\n",
    "      p = p.cpu().numpy()\n",
    " \n",
    "      #get class predictions\n",
    "      #y_pred = som.predict(train_x,train_x,train_y, classes) \n",
    "      y_pred = np.array(y_pred)     \n",
    "      y_pred_last = np.copy(y_pred)\n",
    "\n",
    "      print(\"Initial accuracy km: %.4f\" % met.acc(y, y_pred))\n",
    "      print(\"Initial accuracy dec: %.4f\" % met.acc(y, q.cpu().numpy().argmax(1)))\n",
    "\n",
    "      # Step 2: deep clustering\n",
    "      # logging file\n",
    "      import csv\n",
    "      logfile = open(save_dir + '/dec_log.csv', 'w')\n",
    "      logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss'])\n",
    "      logwriter.writeheader()\n",
    " \n",
    "      loss = 0\n",
    "      index = 0\n",
    "      index_array = np.arange(x.shape[0])\n",
    "      criterion=nn.KLDivLoss(reduction=\"batchmean\")\n",
    "      optimizer=optim.SGD(self.dec.model.parameters(), lr=0.01, momentum=0.9)\n",
    "      \n",
    "      '''print(\"DEC state_dict:\")\n",
    "      for param_tensor in self.dec.model.state_dict():\n",
    "          print(param_tensor, \"\\t\", self.dec.model.state_dict()[param_tensor].size())'''\n",
    " \n",
    "      '''dataset = TensorDataset(x, p)\n",
    "      loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "      loader_iterator = iter(loader)\n",
    "      dataset = TensorDataset(x, p)\n",
    "      loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "      loader_iterator = iter(loader)'''\n",
    "      index = 0\n",
    "      index_array = np.arange(x.shape[0])\n",
    "      train_loss = 0\n",
    "      for ite in range(int(maxiter)):\n",
    "          if ite % update_interval == 0:\n",
    "              q = self.eval_predict(self.dec.model, trainloader) #soft probability predictions on full dataset\n",
    "              p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "              p = p.cpu().numpy()\n",
    "              # evaluate the clustering performance\n",
    "              y_pred = q.cpu().numpy()\n",
    "              y_pred = y_pred.argmax(1)\n",
    "              if y is not None:\n",
    "                  acc = np.round(met.acc(y, y_pred), 5)\n",
    "                  nmi = np.round(met.nmi(y, y_pred), 5)\n",
    "                  ari = np.round(met.ari(y, y_pred), 5)\n",
    "                  loss = np.round(train_loss, 5)\n",
    "                  logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
    "                  logwriter.writerow(logdict)\n",
    "                  print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    " \n",
    "              # check stop criterion\n",
    "              delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "              y_pred_last = np.copy(y_pred)\n",
    "              if ite > 0 and delta_label < tol:\n",
    "                  print('delta_label ', delta_label, '< tol ', tol)\n",
    "                  print('Reached tolerance threshold. Stopping training.')\n",
    "                  logfile.close()\n",
    "                  break\n",
    "          \n",
    "          #Train on one batch\n",
    "          idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "          input, target = torch.Tensor(x[idx]), torch.Tensor(p[idx])\n",
    "          data = input.to(self.device)\n",
    "          #Forward Pass\n",
    "          output=self.dec.model(data)\n",
    "          target = target.to(self.device)\n",
    "          loss=criterion(output.log(), target)\n",
    "          #Backward Pass\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          train_loss = loss.item()\n",
    "          index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0 \n",
    "                  \n",
    "          # save intermediate model\n",
    "          if ite % save_interval == 0:\n",
    "              print('saving model to:', save_dir + '/KoDEC_model_' + str(ite) + '.h5')\n",
    "              torch.save(self.dec.model.state_dict(), save_dir + '/KoDEC_model_' + str(ite) + '.h5')\n",
    " \n",
    "          ite += 1\n",
    "          \n",
    "      # save the trained model\n",
    "      logfile.close()\n",
    "      print('saving model to:', save_dir + '/KoDEC_model_final.h5')\n",
    "      torch.save(self.dec.model.state_dict(), save_dir + '/KoDEC_model_final.h5')\n",
    "      return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1969,
     "status": "ok",
     "timestamp": 1595824377301,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "ZqbUoZtdu1aD"
   },
   "outputs": [],
   "source": [
    "#setting the hyper parameters\n",
    "init = 'glorot_uniform'\n",
    "pretrain_optimizer = 'adam'\n",
    "dataset = 'mnist'\n",
    "batch_size = 2048\n",
    "maxiter = 2e4\n",
    "tol = 0.001\n",
    "save_dir = 'drive/DEC/pytorch/resultsnew'\n",
    " \n",
    "import os\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    " \n",
    "update_interval = 200\n",
    "pretrain_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1223,
     "status": "ok",
     "timestamp": 1595824379151,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "toMGveGIu1aN"
   },
   "outputs": [],
   "source": [
    "dec = DEC_Module(dims=[train_x.shape[-1], 500, 500, 2000, 10], n_clusters=10, init=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 403802,
     "status": "ok",
     "timestamp": 1595824783995,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "e5mVaf2gu1aX",
    "outputId": "80644d5c-2197-4c7b-a7e0-019ad81aef33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Pretraining...\n",
      "======> epoch: 0/300, Loss:0.06911388784646988\n",
      "        |==>  acc: 0.3534,  nmi: 0.2713  <==|\n",
      "======> epoch: 1/300, Loss:0.05832405388355255\n",
      "======> epoch: 2/300, Loss:0.053069014102220535\n",
      "======> epoch: 3/300, Loss:0.043323591351509094\n",
      "======> epoch: 4/300, Loss:0.03700592368841171\n",
      "======> epoch: 5/300, Loss:0.032928407192230225\n",
      "======> epoch: 6/300, Loss:0.030756615102291107\n",
      "======> epoch: 7/300, Loss:0.029836876317858696\n",
      "======> epoch: 8/300, Loss:0.028716588392853737\n",
      "======> epoch: 9/300, Loss:0.02777022123336792\n",
      "======> epoch: 10/300, Loss:0.029802318662405014\n",
      "======> epoch: 11/300, Loss:0.026429975405335426\n",
      "======> epoch: 12/300, Loss:0.02551441080868244\n",
      "======> epoch: 13/300, Loss:0.025005271658301353\n",
      "======> epoch: 14/300, Loss:0.024435052648186684\n",
      "======> epoch: 15/300, Loss:0.023790856823325157\n",
      "======> epoch: 16/300, Loss:0.0233159177005291\n",
      "======> epoch: 17/300, Loss:0.023031534627079964\n",
      "======> epoch: 18/300, Loss:0.022407079115509987\n",
      "======> epoch: 19/300, Loss:0.022244445979595184\n",
      "======> epoch: 20/300, Loss:0.021967271342873573\n",
      "======> epoch: 21/300, Loss:0.02132674865424633\n",
      "======> epoch: 22/300, Loss:0.02068665623664856\n",
      "======> epoch: 23/300, Loss:0.02031802199780941\n",
      "======> epoch: 24/300, Loss:0.020087813958525658\n",
      "======> epoch: 25/300, Loss:0.0199847724288702\n",
      "======> epoch: 26/300, Loss:0.019823459908366203\n",
      "======> epoch: 27/300, Loss:0.019516345113515854\n",
      "======> epoch: 28/300, Loss:0.0192433949559927\n",
      "======> epoch: 29/300, Loss:0.019006803631782532\n",
      "======> epoch: 30/300, Loss:0.01908954419195652\n",
      "        |==>  acc: 0.6253,  nmi: 0.6080  <==|\n",
      "======> epoch: 31/300, Loss:0.019024627283215523\n",
      "======> epoch: 32/300, Loss:0.01879601553082466\n",
      "======> epoch: 33/300, Loss:0.01831706613302231\n",
      "======> epoch: 34/300, Loss:0.018237357959151268\n",
      "======> epoch: 35/300, Loss:0.01781253144145012\n",
      "======> epoch: 36/300, Loss:0.01789558120071888\n",
      "======> epoch: 37/300, Loss:0.01748603954911232\n",
      "======> epoch: 38/300, Loss:0.017277564853429794\n",
      "======> epoch: 39/300, Loss:0.0171060711145401\n",
      "======> epoch: 40/300, Loss:0.01697998307645321\n",
      "======> epoch: 41/300, Loss:0.017091045156121254\n",
      "======> epoch: 42/300, Loss:0.01716136746108532\n",
      "======> epoch: 43/300, Loss:0.017357584089040756\n",
      "======> epoch: 44/300, Loss:0.016749143600463867\n",
      "======> epoch: 45/300, Loss:0.017531320452690125\n",
      "======> epoch: 46/300, Loss:0.01701212115585804\n",
      "======> epoch: 47/300, Loss:0.016518469899892807\n",
      "======> epoch: 48/300, Loss:0.016726842150092125\n",
      "======> epoch: 49/300, Loss:0.016296211630105972\n",
      "======> epoch: 50/300, Loss:0.016037169843912125\n",
      "======> epoch: 51/300, Loss:0.016469616442918777\n",
      "======> epoch: 52/300, Loss:0.01595691218972206\n",
      "======> epoch: 53/300, Loss:0.01583837904036045\n",
      "======> epoch: 54/300, Loss:0.015867233276367188\n",
      "======> epoch: 55/300, Loss:0.01589018665254116\n",
      "======> epoch: 56/300, Loss:0.01567673124372959\n",
      "======> epoch: 57/300, Loss:0.015500959940254688\n",
      "======> epoch: 58/300, Loss:0.01542420219630003\n",
      "======> epoch: 59/300, Loss:0.015616518445312977\n",
      "======> epoch: 60/300, Loss:0.015799937769770622\n",
      "        |==>  acc: 0.6983,  nmi: 0.6461  <==|\n",
      "======> epoch: 61/300, Loss:0.015467338263988495\n",
      "======> epoch: 62/300, Loss:0.015456676483154297\n",
      "======> epoch: 63/300, Loss:0.015709761530160904\n",
      "======> epoch: 64/300, Loss:0.01539140660315752\n",
      "======> epoch: 65/300, Loss:0.015684915706515312\n",
      "======> epoch: 66/300, Loss:0.01505759172141552\n",
      "======> epoch: 67/300, Loss:0.014814243651926517\n",
      "======> epoch: 68/300, Loss:0.014815321192145348\n",
      "======> epoch: 69/300, Loss:0.014657563529908657\n",
      "======> epoch: 70/300, Loss:0.014561786316335201\n",
      "======> epoch: 71/300, Loss:0.014505588449537754\n",
      "======> epoch: 72/300, Loss:0.014585539698600769\n",
      "======> epoch: 73/300, Loss:0.014855961315333843\n",
      "======> epoch: 74/300, Loss:0.014893874526023865\n",
      "======> epoch: 75/300, Loss:0.01475819107145071\n",
      "======> epoch: 76/300, Loss:0.014877356588840485\n",
      "======> epoch: 77/300, Loss:0.0148612130433321\n",
      "======> epoch: 78/300, Loss:0.014635186642408371\n",
      "======> epoch: 79/300, Loss:0.015631763264536858\n",
      "======> epoch: 80/300, Loss:0.014961523935198784\n",
      "======> epoch: 81/300, Loss:0.014736587181687355\n",
      "======> epoch: 82/300, Loss:0.014599082060158253\n",
      "======> epoch: 83/300, Loss:0.014475046657025814\n",
      "======> epoch: 84/300, Loss:0.014460353180766106\n",
      "======> epoch: 85/300, Loss:0.014239626005291939\n",
      "======> epoch: 86/300, Loss:0.014347179792821407\n",
      "======> epoch: 87/300, Loss:0.014253802597522736\n",
      "======> epoch: 88/300, Loss:0.014188650995492935\n",
      "======> epoch: 89/300, Loss:0.014209398999810219\n",
      "======> epoch: 90/300, Loss:0.01407089177519083\n",
      "        |==>  acc: 0.7426,  nmi: 0.6739  <==|\n",
      "======> epoch: 91/300, Loss:0.013877423480153084\n",
      "======> epoch: 92/300, Loss:0.013850989751517773\n",
      "======> epoch: 93/300, Loss:0.013812519609928131\n",
      "======> epoch: 94/300, Loss:0.014245044440031052\n",
      "======> epoch: 95/300, Loss:0.013974475674331188\n",
      "======> epoch: 96/300, Loss:0.015231656841933727\n",
      "======> epoch: 97/300, Loss:0.014930310659110546\n",
      "======> epoch: 98/300, Loss:0.014285102486610413\n",
      "======> epoch: 99/300, Loss:0.014071858488023281\n",
      "======> epoch: 100/300, Loss:0.01383713074028492\n",
      "======> epoch: 101/300, Loss:0.013849764131009579\n",
      "======> epoch: 102/300, Loss:0.013921170495450497\n",
      "======> epoch: 103/300, Loss:0.014257356524467468\n",
      "======> epoch: 104/300, Loss:0.013907883316278458\n",
      "======> epoch: 105/300, Loss:0.01369505189359188\n",
      "======> epoch: 106/300, Loss:0.013522064313292503\n",
      "======> epoch: 107/300, Loss:0.013643452897667885\n",
      "======> epoch: 108/300, Loss:0.013501917012035847\n",
      "======> epoch: 109/300, Loss:0.013598981313407421\n",
      "======> epoch: 110/300, Loss:0.013310845009982586\n",
      "======> epoch: 111/300, Loss:0.013296281918883324\n",
      "======> epoch: 112/300, Loss:0.013422862626612186\n",
      "======> epoch: 113/300, Loss:0.013485914096236229\n",
      "======> epoch: 114/300, Loss:0.013397814705967903\n",
      "======> epoch: 115/300, Loss:0.013600019738078117\n",
      "======> epoch: 116/300, Loss:0.01340800803154707\n",
      "======> epoch: 117/300, Loss:0.013533975929021835\n",
      "======> epoch: 118/300, Loss:0.013744385913014412\n",
      "======> epoch: 119/300, Loss:0.014149602502584457\n",
      "======> epoch: 120/300, Loss:0.013736017048358917\n",
      "        |==>  acc: 0.7646,  nmi: 0.6989  <==|\n",
      "======> epoch: 121/300, Loss:0.013616141863167286\n",
      "======> epoch: 122/300, Loss:0.013416748493909836\n",
      "======> epoch: 123/300, Loss:0.01322366576641798\n",
      "======> epoch: 124/300, Loss:0.013112451881170273\n",
      "======> epoch: 125/300, Loss:0.013230129145085812\n",
      "======> epoch: 126/300, Loss:0.013115975074470043\n",
      "======> epoch: 127/300, Loss:0.013044475577771664\n",
      "======> epoch: 128/300, Loss:0.013179883360862732\n",
      "======> epoch: 129/300, Loss:0.013041418977081776\n",
      "======> epoch: 130/300, Loss:0.013144797645509243\n",
      "======> epoch: 131/300, Loss:0.013287872076034546\n",
      "======> epoch: 132/300, Loss:0.013238280080258846\n",
      "======> epoch: 133/300, Loss:0.013156797736883163\n",
      "======> epoch: 134/300, Loss:0.013142748735845089\n",
      "======> epoch: 135/300, Loss:0.013130088336765766\n",
      "======> epoch: 136/300, Loss:0.013036277145147324\n",
      "======> epoch: 137/300, Loss:0.01321810856461525\n",
      "======> epoch: 138/300, Loss:0.013275875709950924\n",
      "======> epoch: 139/300, Loss:0.014016165398061275\n",
      "======> epoch: 140/300, Loss:0.014609876088798046\n",
      "======> epoch: 141/300, Loss:0.01342650968581438\n",
      "======> epoch: 142/300, Loss:0.013412589207291603\n",
      "======> epoch: 143/300, Loss:0.013367898762226105\n",
      "======> epoch: 144/300, Loss:0.012875518761575222\n",
      "======> epoch: 145/300, Loss:0.012686483561992645\n",
      "======> epoch: 146/300, Loss:0.012714945711195469\n",
      "======> epoch: 147/300, Loss:0.013402141630649567\n",
      "======> epoch: 148/300, Loss:0.013150304555892944\n",
      "======> epoch: 149/300, Loss:0.013235040009021759\n",
      "======> epoch: 150/300, Loss:0.012768551707267761\n",
      "        |==>  acc: 0.7738,  nmi: 0.7063  <==|\n",
      "======> epoch: 151/300, Loss:0.013203647918999195\n",
      "======> epoch: 152/300, Loss:0.013494016602635384\n",
      "======> epoch: 153/300, Loss:0.013120408169925213\n",
      "======> epoch: 154/300, Loss:0.012823635712265968\n",
      "======> epoch: 155/300, Loss:0.012724773958325386\n",
      "======> epoch: 156/300, Loss:0.013031517155468464\n",
      "======> epoch: 157/300, Loss:0.013070967048406601\n",
      "======> epoch: 158/300, Loss:0.012490104883909225\n",
      "======> epoch: 159/300, Loss:0.012394174002110958\n",
      "======> epoch: 160/300, Loss:0.012357239611446857\n",
      "======> epoch: 161/300, Loss:0.012510095722973347\n",
      "======> epoch: 162/300, Loss:0.012694837525486946\n",
      "======> epoch: 163/300, Loss:0.012990646995604038\n",
      "======> epoch: 164/300, Loss:0.01309993490576744\n",
      "======> epoch: 165/300, Loss:0.013024413958191872\n",
      "======> epoch: 166/300, Loss:0.013617479242384434\n",
      "======> epoch: 167/300, Loss:0.012692954391241074\n",
      "======> epoch: 168/300, Loss:0.012503166683018208\n",
      "======> epoch: 169/300, Loss:0.012546579353511333\n",
      "======> epoch: 170/300, Loss:0.012370229698717594\n",
      "======> epoch: 171/300, Loss:0.012057272717356682\n",
      "======> epoch: 172/300, Loss:0.011970477178692818\n",
      "======> epoch: 173/300, Loss:0.011840099468827248\n",
      "======> epoch: 174/300, Loss:0.011808170937001705\n",
      "======> epoch: 175/300, Loss:0.011860295198857784\n",
      "======> epoch: 176/300, Loss:0.011933908797800541\n",
      "======> epoch: 177/300, Loss:0.01185619831085205\n",
      "======> epoch: 178/300, Loss:0.012565518729388714\n",
      "======> epoch: 179/300, Loss:0.012044117785990238\n",
      "======> epoch: 180/300, Loss:0.012041923590004444\n",
      "        |==>  acc: 0.7781,  nmi: 0.7018  <==|\n",
      "======> epoch: 181/300, Loss:0.011940532363951206\n",
      "======> epoch: 182/300, Loss:0.012339930981397629\n",
      "======> epoch: 183/300, Loss:0.0123082110658288\n",
      "======> epoch: 184/300, Loss:0.01252464298158884\n",
      "======> epoch: 185/300, Loss:0.012726950459182262\n",
      "======> epoch: 186/300, Loss:0.012813362292945385\n",
      "======> epoch: 187/300, Loss:0.012228596955537796\n",
      "======> epoch: 188/300, Loss:0.012176473625004292\n",
      "======> epoch: 189/300, Loss:0.012317640706896782\n",
      "======> epoch: 190/300, Loss:0.012837259098887444\n",
      "======> epoch: 191/300, Loss:0.013377057388424873\n",
      "======> epoch: 192/300, Loss:0.012269611470401287\n",
      "======> epoch: 193/300, Loss:0.011921023949980736\n",
      "======> epoch: 194/300, Loss:0.011882815510034561\n",
      "======> epoch: 195/300, Loss:0.011730683967471123\n",
      "======> epoch: 196/300, Loss:0.011890885420143604\n",
      "======> epoch: 197/300, Loss:0.012119967490434647\n",
      "======> epoch: 198/300, Loss:0.01265615876764059\n",
      "======> epoch: 199/300, Loss:0.012378876097500324\n",
      "======> epoch: 200/300, Loss:0.012445504777133465\n",
      "======> epoch: 201/300, Loss:0.011932569555938244\n",
      "======> epoch: 202/300, Loss:0.011892510578036308\n",
      "======> epoch: 203/300, Loss:0.011834214441478252\n",
      "======> epoch: 204/300, Loss:0.012365126982331276\n",
      "======> epoch: 205/300, Loss:0.012155492790043354\n",
      "======> epoch: 206/300, Loss:0.012314124964177608\n",
      "======> epoch: 207/300, Loss:0.011998187750577927\n",
      "======> epoch: 208/300, Loss:0.011835383251309395\n",
      "======> epoch: 209/300, Loss:0.01190983597189188\n",
      "======> epoch: 210/300, Loss:0.011955564841628075\n",
      "        |==>  acc: 0.7812,  nmi: 0.7017  <==|\n",
      "======> epoch: 211/300, Loss:0.01206060592085123\n",
      "======> epoch: 212/300, Loss:0.0119175398722291\n",
      "======> epoch: 213/300, Loss:0.01203894056379795\n",
      "======> epoch: 214/300, Loss:0.012169651687145233\n",
      "======> epoch: 215/300, Loss:0.011889339424669743\n",
      "======> epoch: 216/300, Loss:0.011544815264642239\n",
      "======> epoch: 217/300, Loss:0.011390883475542068\n",
      "======> epoch: 218/300, Loss:0.01146660652011633\n",
      "======> epoch: 219/300, Loss:0.011700796894729137\n",
      "======> epoch: 220/300, Loss:0.012228980660438538\n",
      "======> epoch: 221/300, Loss:0.012046108953654766\n",
      "======> epoch: 222/300, Loss:0.011893198825418949\n",
      "======> epoch: 223/300, Loss:0.011781358160078526\n",
      "======> epoch: 224/300, Loss:0.011681715026497841\n",
      "======> epoch: 225/300, Loss:0.011730466969311237\n",
      "======> epoch: 226/300, Loss:0.01145357545465231\n",
      "======> epoch: 227/300, Loss:0.011453532613813877\n",
      "======> epoch: 228/300, Loss:0.011751381680369377\n",
      "======> epoch: 229/300, Loss:0.012501978315412998\n",
      "======> epoch: 230/300, Loss:0.012934685684740543\n",
      "======> epoch: 231/300, Loss:0.012407499365508556\n",
      "======> epoch: 232/300, Loss:0.011907531879842281\n",
      "======> epoch: 233/300, Loss:0.011794577352702618\n",
      "======> epoch: 234/300, Loss:0.011585836298763752\n",
      "======> epoch: 235/300, Loss:0.011706873774528503\n",
      "======> epoch: 236/300, Loss:0.011764238588511944\n",
      "======> epoch: 237/300, Loss:0.011478994973003864\n",
      "======> epoch: 238/300, Loss:0.011440683156251907\n",
      "======> epoch: 239/300, Loss:0.011729407124221325\n",
      "======> epoch: 240/300, Loss:0.011493928730487823\n",
      "        |==>  acc: 0.7726,  nmi: 0.6976  <==|\n",
      "======> epoch: 241/300, Loss:0.011554188095033169\n",
      "======> epoch: 242/300, Loss:0.011441641487181187\n",
      "======> epoch: 243/300, Loss:0.011222555302083492\n",
      "======> epoch: 244/300, Loss:0.011365022510290146\n",
      "======> epoch: 245/300, Loss:0.01147384475916624\n",
      "======> epoch: 246/300, Loss:0.011395004577934742\n",
      "======> epoch: 247/300, Loss:0.011461044661700726\n",
      "======> epoch: 248/300, Loss:0.011524975299835205\n",
      "======> epoch: 249/300, Loss:0.0117102712392807\n",
      "======> epoch: 250/300, Loss:0.011598748154938221\n",
      "======> epoch: 251/300, Loss:0.011256327852606773\n",
      "======> epoch: 252/300, Loss:0.011602194048464298\n",
      "======> epoch: 253/300, Loss:0.011303068138659\n",
      "======> epoch: 254/300, Loss:0.01084723137319088\n",
      "======> epoch: 255/300, Loss:0.010662387125194073\n",
      "======> epoch: 256/300, Loss:0.010612149722874165\n",
      "======> epoch: 257/300, Loss:0.010595228523015976\n",
      "======> epoch: 258/300, Loss:0.010632035322487354\n",
      "======> epoch: 259/300, Loss:0.010727344080805779\n",
      "======> epoch: 260/300, Loss:0.011273386888206005\n",
      "======> epoch: 261/300, Loss:0.011399555020034313\n",
      "======> epoch: 262/300, Loss:0.011603154242038727\n",
      "======> epoch: 263/300, Loss:0.011194116435945034\n",
      "======> epoch: 264/300, Loss:0.010959784500300884\n",
      "======> epoch: 265/300, Loss:0.010830627754330635\n",
      "======> epoch: 266/300, Loss:0.010778343304991722\n",
      "======> epoch: 267/300, Loss:0.010790057480335236\n",
      "======> epoch: 268/300, Loss:0.011050919070839882\n",
      "======> epoch: 269/300, Loss:0.01134660467505455\n",
      "======> epoch: 270/300, Loss:0.011704222299158573\n",
      "        |==>  acc: 0.7858,  nmi: 0.7090  <==|\n",
      "======> epoch: 271/300, Loss:0.011336811818182468\n",
      "======> epoch: 272/300, Loss:0.01093253307044506\n",
      "======> epoch: 273/300, Loss:0.01099089439958334\n",
      "======> epoch: 274/300, Loss:0.011417767964303493\n",
      "======> epoch: 275/300, Loss:0.01154304388910532\n",
      "======> epoch: 276/300, Loss:0.01105924416333437\n",
      "======> epoch: 277/300, Loss:0.010905716568231583\n",
      "======> epoch: 278/300, Loss:0.010794518515467644\n",
      "======> epoch: 279/300, Loss:0.010843479074537754\n",
      "======> epoch: 280/300, Loss:0.010789069347083569\n",
      "======> epoch: 281/300, Loss:0.011016798205673695\n",
      "======> epoch: 282/300, Loss:0.01155144814401865\n",
      "======> epoch: 283/300, Loss:0.011338037438690662\n",
      "======> epoch: 284/300, Loss:0.011034717783331871\n",
      "======> epoch: 285/300, Loss:0.0107884481549263\n",
      "======> epoch: 286/300, Loss:0.010621899738907814\n",
      "======> epoch: 287/300, Loss:0.010673741810023785\n",
      "======> epoch: 288/300, Loss:0.01093387883156538\n",
      "======> epoch: 289/300, Loss:0.010825489647686481\n",
      "======> epoch: 290/300, Loss:0.011149986647069454\n",
      "======> epoch: 291/300, Loss:0.010880735702812672\n",
      "======> epoch: 292/300, Loss:0.011142122559249401\n",
      "======> epoch: 293/300, Loss:0.011729113757610321\n",
      "======> epoch: 294/300, Loss:0.011400829069316387\n",
      "======> epoch: 295/300, Loss:0.01104083564132452\n",
      "======> epoch: 296/300, Loss:0.010962328873574734\n",
      "======> epoch: 297/300, Loss:0.011010640300810337\n",
      "======> epoch: 298/300, Loss:0.011236254125833511\n",
      "======> epoch: 299/300, Loss:0.011821472086012363\n",
      "Pretraining time:  386.3398370742798\n",
      "Pretrained weights are saved to drive/DEC/pytorch/resultsnew/encoder_weights.h5\n"
     ]
    }
   ],
   "source": [
    "dec.pretrain(x=train_x, y=train_y, optimizer=pretrain_optimizer,\n",
    "             epochs=pretrain_epochs, batch_size=batch_size,\n",
    "             save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2808,
     "status": "ok",
     "timestamp": 1595824919281,
     "user": {
      "displayName": "Ogban Ugot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjJyWq0J1XhvgJDqKNrelkF1Cze3R1KJUGNz2KwQg=s64",
      "userId": "10065313831802116021"
     },
     "user_tz": -60
    },
    "id": "0niwZo2vBz5O",
    "outputId": "9f69cfa2-9c64-4036-9afc-ca48205dd4c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.dec.model.encoder.load_state_dict(torch.load(\"drive/DEC/pytorch/resultsnew/encoder_weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dec.train(train_x, y=train_y, tol=tol, maxiter=maxiter, batch_size=batch_size,\n",
    "                 update_interval=update_interval, save_dir=save_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DEC_Pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
